{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-24 15:56:06.257284: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-24 15:56:06.785701: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-24 15:56:06.785760: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-24 15:56:06.785806: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-24 15:56:06.884895: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-24 15:56:06.886297: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-24 15:56:19.387601: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config IPCompleter.greedy=True\n",
    "\n",
    "import sys, os, time, warnings, pdb, pickle, random, math, re, json\n",
    "warnings.filterwarnings('ignore')\n",
    "sys.path.insert(0, '../scripts')\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Flatten, Concatenate, Dropout, CategoryEncoding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import metrics\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datetime import datetime\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "sns.set_style(\"darkgrid\")\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('fraudTest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Engineering\n",
    "df['age'] = 2020 - df['dob'].apply(lambda x: int(x[:4]))\n",
    "df['unix_time'] = df['unix_time'] - 1371816865\n",
    "df['hour'] = df['unix_time'].apply(lambda x: datetime.utcfromtimestamp(x).hour)\n",
    "df['gender'] = df['gender'].apply(lambda x: 1 if x == 'M' else 0)\n",
    "\n",
    "df.drop(['Unnamed: 0', 'trans_date_trans_time', 'dob', 'first', 'last', 'street', 'state', 'city', 'zip', 'trans_num'], axis=1, inplace=True)\n",
    "\n",
    "# Initialize LabelEncoder for nameOrig, nameDest, and type\n",
    "label_encoder_cc = LabelEncoder()\n",
    "label_encoder_merchant = LabelEncoder()\n",
    "label_encoder_job = LabelEncoder()\n",
    "label_encoder_category = LabelEncoder()\n",
    "\n",
    "# Fit the encoder and transform the nameOrig and nameDest columns\n",
    "df['cc_num'] = label_encoder_cc.fit_transform(df['cc_num'])\n",
    "df['merchant'] = label_encoder_merchant.fit_transform(df['merchant'])\n",
    "df['job'] = label_encoder_job.fit_transform(df['job'])\n",
    "df['category'] = label_encoder_category.fit_transform(df['category'])\n",
    "\n",
    "val_df = df.iloc[int(len(df)*0.9):]\n",
    "df = df.iloc[:int(len(df)*0.9)]\n",
    "\n",
    "# Train-test split\n",
    "y = df['is_fraud']\n",
    "X = df.drop(['is_fraud'], axis=1)\n",
    "y_val = val_df['is_fraud']\n",
    "X_val = val_df.drop(['is_fraud'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train[['amt', 'lat', 'long', 'city_pop', 'unix_time', 'merch_lat', 'merch_long', 'age', 'hour']] = scaler.fit_transform(\n",
    "    X_train[['amt', 'lat', 'long', 'city_pop', 'unix_time', 'merch_lat', 'merch_long', 'age', 'hour']])\n",
    "X_test[['amt', 'lat', 'long', 'city_pop', 'unix_time', 'merch_lat', 'merch_long', 'age', 'hour']] = scaler.transform(\n",
    "    X_test[['amt', 'lat', 'long', 'city_pop', 'unix_time', 'merch_lat', 'merch_long', 'age', 'hour']])\n",
    "val_df[['amt', 'lat', 'long', 'city_pop', 'unix_time', 'merch_lat', 'merch_long', 'age', 'hour']] = scaler.transform(\n",
    "    val_df[['amt', 'lat', 'long', 'city_pop', 'unix_time', 'merch_lat', 'merch_long', 'age', 'hour']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cc_num</th>\n",
       "      <th>merchant</th>\n",
       "      <th>category</th>\n",
       "      <th>amt</th>\n",
       "      <th>gender</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>city_pop</th>\n",
       "      <th>job</th>\n",
       "      <th>unix_time</th>\n",
       "      <th>merch_lat</th>\n",
       "      <th>merch_long</th>\n",
       "      <th>age</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4414</th>\n",
       "      <td>309</td>\n",
       "      <td>564</td>\n",
       "      <td>10</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.19</td>\n",
       "      <td>0.64</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>171</td>\n",
       "      <td>-1.64</td>\n",
       "      <td>-2.32</td>\n",
       "      <td>0.71</td>\n",
       "      <td>-0.73</td>\n",
       "      <td>-1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109326</th>\n",
       "      <td>652</td>\n",
       "      <td>333</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>1</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-2.16</td>\n",
       "      <td>0.62</td>\n",
       "      <td>233</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-2.19</td>\n",
       "      <td>-1.36</td>\n",
       "      <td>-0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219396</th>\n",
       "      <td>279</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.05</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>332</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284761</th>\n",
       "      <td>237</td>\n",
       "      <td>165</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>169</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-1.01</td>\n",
       "      <td>-0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464782</th>\n",
       "      <td>667</td>\n",
       "      <td>691</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>303</td>\n",
       "      <td>1.43</td>\n",
       "      <td>-0.79</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380802</th>\n",
       "      <td>348</td>\n",
       "      <td>594</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>1</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>333</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96527</th>\n",
       "      <td>638</td>\n",
       "      <td>452</td>\n",
       "      <td>9</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.66</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>325</td>\n",
       "      <td>-1.06</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28890</th>\n",
       "      <td>119</td>\n",
       "      <td>166</td>\n",
       "      <td>5</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>-1.30</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>229</td>\n",
       "      <td>-1.50</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>-1.31</td>\n",
       "      <td>-0.90</td>\n",
       "      <td>-0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438472</th>\n",
       "      <td>738</td>\n",
       "      <td>552</td>\n",
       "      <td>6</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.13</td>\n",
       "      <td>0.51</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>186</td>\n",
       "      <td>1.33</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>0.47</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-1.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86184</th>\n",
       "      <td>341</td>\n",
       "      <td>625</td>\n",
       "      <td>2</td>\n",
       "      <td>0.11</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.15</td>\n",
       "      <td>-1.96</td>\n",
       "      <td>3.81</td>\n",
       "      <td>97</td>\n",
       "      <td>-1.14</td>\n",
       "      <td>-1.07</td>\n",
       "      <td>-1.93</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400117 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        cc_num  merchant  category   amt  gender   lat  long  city_pop  job  \\\n",
       "4414       309       564        10  0.48       0 -2.19  0.64     -0.13  171   \n",
       "109326     652       333         6 -0.28       1  0.20 -2.16      0.62  233   \n",
       "219396     279        20         4  0.33       1  0.17  1.05     -0.29  332   \n",
       "284761     237       165         1 -0.40       0  0.22  0.03     -0.29  169   \n",
       "464782     667       691         2 -0.10       0 -0.78 -0.38     -0.29  303   \n",
       "...        ...       ...       ...   ...     ...   ...   ...       ...  ...   \n",
       "380802     348       594        11 -0.45       1  0.73  0.50     -0.19  333   \n",
       "96527      638       452         9 -0.41       0 -0.66 -0.14     -0.29  325   \n",
       "28890      119       166         5  0.41       1 -0.56 -1.30     -0.27  229   \n",
       "438472     738       552         6  0.04       1 -1.13  0.51     -0.29  186   \n",
       "86184      341       625         2  0.11       1 -1.15 -1.96      3.81   97   \n",
       "\n",
       "        unix_time  merch_lat  merch_long   age  hour  \n",
       "4414        -1.64      -2.32        0.71 -0.73 -1.06  \n",
       "109326      -1.00       0.10       -2.19 -1.36 -0.18  \n",
       "219396      -0.27       0.29        1.11  0.25  1.87  \n",
       "284761       0.23       0.14        0.01 -1.01 -0.62  \n",
       "464782       1.43      -0.79       -0.41  0.08  0.55  \n",
       "...           ...        ...         ...   ...   ...  \n",
       "380802       0.98       0.54        0.46  0.13 -0.33  \n",
       "96527       -1.06      -0.70       -0.13  1.11  0.70  \n",
       "28890       -1.50      -0.48       -1.31 -0.90 -0.77  \n",
       "438472       1.33      -0.94        0.47 -0.10 -1.50  \n",
       "86184       -1.14      -1.07       -1.93 -0.78  0.26  \n",
       "\n",
       "[400117 rows x 14 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cc_num</th>\n",
       "      <th>merchant</th>\n",
       "      <th>category</th>\n",
       "      <th>amt</th>\n",
       "      <th>gender</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>city_pop</th>\n",
       "      <th>job</th>\n",
       "      <th>unix_time</th>\n",
       "      <th>merch_lat</th>\n",
       "      <th>merch_long</th>\n",
       "      <th>age</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4414</th>\n",
       "      <td>309</td>\n",
       "      <td>564</td>\n",
       "      <td>10</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.19</td>\n",
       "      <td>0.64</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>171</td>\n",
       "      <td>-1.64</td>\n",
       "      <td>-2.32</td>\n",
       "      <td>0.71</td>\n",
       "      <td>-0.73</td>\n",
       "      <td>-1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109326</th>\n",
       "      <td>652</td>\n",
       "      <td>333</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>1</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-2.16</td>\n",
       "      <td>0.62</td>\n",
       "      <td>233</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-2.19</td>\n",
       "      <td>-1.36</td>\n",
       "      <td>-0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219396</th>\n",
       "      <td>279</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.05</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>332</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284761</th>\n",
       "      <td>237</td>\n",
       "      <td>165</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>169</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-1.01</td>\n",
       "      <td>-0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464782</th>\n",
       "      <td>667</td>\n",
       "      <td>691</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>303</td>\n",
       "      <td>1.43</td>\n",
       "      <td>-0.79</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380802</th>\n",
       "      <td>348</td>\n",
       "      <td>594</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>1</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>333</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96527</th>\n",
       "      <td>638</td>\n",
       "      <td>452</td>\n",
       "      <td>9</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.66</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>325</td>\n",
       "      <td>-1.06</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28890</th>\n",
       "      <td>119</td>\n",
       "      <td>166</td>\n",
       "      <td>5</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>-1.30</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>229</td>\n",
       "      <td>-1.50</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>-1.31</td>\n",
       "      <td>-0.90</td>\n",
       "      <td>-0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438472</th>\n",
       "      <td>738</td>\n",
       "      <td>552</td>\n",
       "      <td>6</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.13</td>\n",
       "      <td>0.51</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>186</td>\n",
       "      <td>1.33</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>0.47</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-1.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86184</th>\n",
       "      <td>341</td>\n",
       "      <td>625</td>\n",
       "      <td>2</td>\n",
       "      <td>0.11</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.15</td>\n",
       "      <td>-1.96</td>\n",
       "      <td>3.81</td>\n",
       "      <td>97</td>\n",
       "      <td>-1.14</td>\n",
       "      <td>-1.07</td>\n",
       "      <td>-1.93</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400117 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        cc_num  merchant  category   amt  gender   lat  long  city_pop  job  \\\n",
       "4414       309       564        10  0.48       0 -2.19  0.64     -0.13  171   \n",
       "109326     652       333         6 -0.28       1  0.20 -2.16      0.62  233   \n",
       "219396     279        20         4  0.33       1  0.17  1.05     -0.29  332   \n",
       "284761     237       165         1 -0.40       0  0.22  0.03     -0.29  169   \n",
       "464782     667       691         2 -0.10       0 -0.78 -0.38     -0.29  303   \n",
       "...        ...       ...       ...   ...     ...   ...   ...       ...  ...   \n",
       "380802     348       594        11 -0.45       1  0.73  0.50     -0.19  333   \n",
       "96527      638       452         9 -0.41       0 -0.66 -0.14     -0.29  325   \n",
       "28890      119       166         5  0.41       1 -0.56 -1.30     -0.27  229   \n",
       "438472     738       552         6  0.04       1 -1.13  0.51     -0.29  186   \n",
       "86184      341       625         2  0.11       1 -1.15 -1.96      3.81   97   \n",
       "\n",
       "        unix_time  merch_lat  merch_long   age  hour  \n",
       "4414        -1.64      -2.32        0.71 -0.73 -1.06  \n",
       "109326      -1.00       0.10       -2.19 -1.36 -0.18  \n",
       "219396      -0.27       0.29        1.11  0.25  1.87  \n",
       "284761       0.23       0.14        0.01 -1.01 -0.62  \n",
       "464782       1.43      -0.79       -0.41  0.08  0.55  \n",
       "...           ...        ...         ...   ...   ...  \n",
       "380802       0.98       0.54        0.46  0.13 -0.33  \n",
       "96527       -1.06      -0.70       -0.13  1.11  0.70  \n",
       "28890       -1.50      -0.48       -1.31 -0.90 -0.77  \n",
       "438472       1.33      -0.94        0.47 -0.10 -1.50  \n",
       "86184       -1.14      -1.07       -1.93 -0.78  0.26  \n",
       "\n",
       "[400117 rows x 14 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-24 15:57:58.720997: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-24 15:57:58.721620: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-24 15:57:58.722160: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-24 15:57:58.722733: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-24 15:57:58.723266: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-24 15:57:58.723785: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-24 15:57:59.425332: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2211] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "13/13 [==============================] - 3s 109ms/step - loss: 0.4356 - cross entropy: 0.4119 - Brier score: 0.1116 - tp: 1257.0000 - fp: 50007.0000 - tn: 348411.0000 - fn: 442.0000 - accuracy: 0.8739 - precision: 0.0245 - recall: 0.7398 - auc: 0.8707 - prc: 0.1460 - val_loss: 0.2720 - val_cross entropy: 0.2720 - val_Brier score: 0.0590 - val_tp: 314.0000 - val_fp: 2025.0000 - val_tn: 97580.0000 - val_fn: 111.0000 - val_accuracy: 0.9786 - val_precision: 0.1342 - val_recall: 0.7388 - val_auc: 0.9506 - val_prc: 0.2197\n",
      "Epoch 2/500\n",
      "13/13 [==============================] - 1s 69ms/step - loss: 0.2738 - cross entropy: 0.2336 - Brier score: 0.0540 - tp: 1344.0000 - fp: 18134.0000 - tn: 380284.0000 - fn: 355.0000 - accuracy: 0.9538 - precision: 0.0690 - recall: 0.7911 - auc: 0.9605 - prc: 0.2397 - val_loss: 0.1754 - val_cross entropy: 0.1754 - val_Brier score: 0.0469 - val_tp: 376.0000 - val_fp: 7034.0000 - val_tn: 92571.0000 - val_fn: 49.0000 - val_accuracy: 0.9292 - val_precision: 0.0507 - val_recall: 0.8847 - val_auc: 0.9761 - val_prc: 0.3224\n",
      "Epoch 3/500\n",
      "13/13 [==============================] - 1s 70ms/step - loss: 0.1814 - cross entropy: 0.1908 - Brier score: 0.0555 - tp: 1612.0000 - fp: 34218.0000 - tn: 364200.0000 - fn: 87.0000 - accuracy: 0.9143 - precision: 0.0450 - recall: 0.9488 - auc: 0.9806 - prc: 0.3052 - val_loss: 0.1752 - val_cross entropy: 0.1752 - val_Brier score: 0.0530 - val_tp: 370.0000 - val_fp: 8294.0000 - val_tn: 91311.0000 - val_fn: 55.0000 - val_accuracy: 0.9165 - val_precision: 0.0427 - val_recall: 0.8706 - val_auc: 0.9721 - val_prc: 0.3651\n",
      "Epoch 4/500\n",
      "13/13 [==============================] - 1s 67ms/step - loss: 0.1511 - cross entropy: 0.1615 - Brier score: 0.0488 - tp: 1628.0000 - fp: 30391.0000 - tn: 368027.0000 - fn: 71.0000 - accuracy: 0.9239 - precision: 0.0508 - recall: 0.9582 - auc: 0.9855 - prc: 0.3248 - val_loss: 0.1707 - val_cross entropy: 0.1707 - val_Brier score: 0.0531 - val_tp: 369.0000 - val_fp: 8434.0000 - val_tn: 91171.0000 - val_fn: 56.0000 - val_accuracy: 0.9151 - val_precision: 0.0419 - val_recall: 0.8682 - val_auc: 0.9672 - val_prc: 0.3816\n",
      "Epoch 5/500\n",
      "13/13 [==============================] - 1s 68ms/step - loss: 0.1382 - cross entropy: 0.1609 - Brier score: 0.0512 - tp: 1647.0000 - fp: 33330.0000 - tn: 365088.0000 - fn: 52.0000 - accuracy: 0.9166 - precision: 0.0471 - recall: 0.9694 - auc: 0.9874 - prc: 0.3748 - val_loss: 0.1453 - val_cross entropy: 0.1453 - val_Brier score: 0.0437 - val_tp: 362.0000 - val_fp: 7159.0000 - val_tn: 92446.0000 - val_fn: 63.0000 - val_accuracy: 0.9278 - val_precision: 0.0481 - val_recall: 0.8518 - val_auc: 0.9513 - val_prc: 0.3966\n",
      "Epoch 6/500\n",
      "13/13 [==============================] - 1s 66ms/step - loss: 0.1199 - cross entropy: 0.1439 - Brier score: 0.0449 - tp: 1652.0000 - fp: 28214.0000 - tn: 370204.0000 - fn: 47.0000 - accuracy: 0.9294 - precision: 0.0553 - recall: 0.9723 - auc: 0.9902 - prc: 0.4241 - val_loss: 0.1392 - val_cross entropy: 0.1392 - val_Brier score: 0.0450 - val_tp: 370.0000 - val_fp: 7005.0000 - val_tn: 92600.0000 - val_fn: 55.0000 - val_accuracy: 0.9294 - val_precision: 0.0502 - val_recall: 0.8706 - val_auc: 0.9529 - val_prc: 0.4763\n",
      "Epoch 7/500\n",
      "13/13 [==============================] - 1s 67ms/step - loss: 0.1090 - cross entropy: 0.1249 - Brier score: 0.0388 - tp: 1657.0000 - fp: 23775.0000 - tn: 374643.0000 - fn: 42.0000 - accuracy: 0.9405 - precision: 0.0652 - recall: 0.9753 - auc: 0.9915 - prc: 0.4556 - val_loss: 0.1399 - val_cross entropy: 0.1399 - val_Brier score: 0.0452 - val_tp: 373.0000 - val_fp: 6849.0000 - val_tn: 92756.0000 - val_fn: 52.0000 - val_accuracy: 0.9310 - val_precision: 0.0516 - val_recall: 0.8776 - val_auc: 0.9505 - val_prc: 0.4839\n",
      "Epoch 8/500\n",
      "13/13 [==============================] - 1s 64ms/step - loss: 0.1017 - cross entropy: 0.1184 - Brier score: 0.0379 - tp: 1667.0000 - fp: 23516.0000 - tn: 374902.0000 - fn: 32.0000 - accuracy: 0.9411 - precision: 0.0662 - recall: 0.9812 - auc: 0.9926 - prc: 0.4909 - val_loss: 0.1299 - val_cross entropy: 0.1299 - val_Brier score: 0.0418 - val_tp: 374.0000 - val_fp: 6357.0000 - val_tn: 93248.0000 - val_fn: 51.0000 - val_accuracy: 0.9359 - val_precision: 0.0556 - val_recall: 0.8800 - val_auc: 0.9482 - val_prc: 0.5045\n",
      "Epoch 9/500\n",
      "13/13 [==============================] - 1s 61ms/step - loss: 0.0917 - cross entropy: 0.1075 - Brier score: 0.0344 - tp: 1668.0000 - fp: 21441.0000 - tn: 376977.0000 - fn: 31.0000 - accuracy: 0.9463 - precision: 0.0722 - recall: 0.9818 - auc: 0.9937 - prc: 0.5288 - val_loss: 0.1023 - val_cross entropy: 0.1023 - val_Brier score: 0.0317 - val_tp: 370.0000 - val_fp: 4801.0000 - val_tn: 94804.0000 - val_fn: 55.0000 - val_accuracy: 0.9515 - val_precision: 0.0716 - val_recall: 0.8706 - val_auc: 0.9406 - val_prc: 0.5203\n",
      "Epoch 10/500\n",
      "13/13 [==============================] - 1s 66ms/step - loss: 0.0797 - cross entropy: 0.1006 - Brier score: 0.0314 - tp: 1680.0000 - fp: 18979.0000 - tn: 379439.0000 - fn: 19.0000 - accuracy: 0.9525 - precision: 0.0813 - recall: 0.9888 - auc: 0.9951 - prc: 0.5353 - val_loss: 0.0929 - val_cross entropy: 0.0929 - val_Brier score: 0.0277 - val_tp: 369.0000 - val_fp: 4176.0000 - val_tn: 95429.0000 - val_fn: 56.0000 - val_accuracy: 0.9577 - val_precision: 0.0812 - val_recall: 0.8682 - val_auc: 0.9370 - val_prc: 0.5146\n",
      "Epoch 11/500\n",
      "13/13 [==============================] - 1s 62ms/step - loss: 0.0773 - cross entropy: 0.0961 - Brier score: 0.0292 - tp: 1674.0000 - fp: 17117.0000 - tn: 381301.0000 - fn: 25.0000 - accuracy: 0.9572 - precision: 0.0891 - recall: 0.9853 - auc: 0.9952 - prc: 0.5324 - val_loss: 0.0885 - val_cross entropy: 0.0885 - val_Brier score: 0.0271 - val_tp: 364.0000 - val_fp: 4034.0000 - val_tn: 95571.0000 - val_fn: 61.0000 - val_accuracy: 0.9591 - val_precision: 0.0828 - val_recall: 0.8565 - val_auc: 0.9388 - val_prc: 0.5572\n",
      "Epoch 12/500\n",
      "13/13 [==============================] - 1s 63ms/step - loss: 0.0729 - cross entropy: 0.0842 - Brier score: 0.0262 - tp: 1671.0000 - fp: 15978.0000 - tn: 382440.0000 - fn: 28.0000 - accuracy: 0.9600 - precision: 0.0947 - recall: 0.9835 - auc: 0.9960 - prc: 0.5907 - val_loss: 0.0859 - val_cross entropy: 0.0859 - val_Brier score: 0.0268 - val_tp: 368.0000 - val_fp: 4108.0000 - val_tn: 95497.0000 - val_fn: 57.0000 - val_accuracy: 0.9584 - val_precision: 0.0822 - val_recall: 0.8659 - val_auc: 0.9427 - val_prc: 0.5831\n",
      "Epoch 13/500\n",
      "13/13 [==============================] - 1s 69ms/step - loss: 0.0694 - cross entropy: 0.0868 - Brier score: 0.0272 - tp: 1676.0000 - fp: 16624.0000 - tn: 381794.0000 - fn: 23.0000 - accuracy: 0.9584 - precision: 0.0916 - recall: 0.9865 - auc: 0.9962 - prc: 0.5914 - val_loss: 0.0813 - val_cross entropy: 0.0813 - val_Brier score: 0.0249 - val_tp: 364.0000 - val_fp: 3823.0000 - val_tn: 95782.0000 - val_fn: 61.0000 - val_accuracy: 0.9612 - val_precision: 0.0869 - val_recall: 0.8565 - val_auc: 0.9401 - val_prc: 0.5939\n",
      "Epoch 14/500\n",
      "13/13 [==============================] - 1s 66ms/step - loss: 0.0634 - cross entropy: 0.0766 - Brier score: 0.0238 - tp: 1680.0000 - fp: 14437.0000 - tn: 383981.0000 - fn: 19.0000 - accuracy: 0.9639 - precision: 0.1042 - recall: 0.9888 - auc: 0.9968 - prc: 0.6289 - val_loss: 0.0801 - val_cross entropy: 0.0801 - val_Brier score: 0.0238 - val_tp: 360.0000 - val_fp: 3495.0000 - val_tn: 96110.0000 - val_fn: 65.0000 - val_accuracy: 0.9644 - val_precision: 0.0934 - val_recall: 0.8471 - val_auc: 0.9384 - val_prc: 0.5793\n",
      "Epoch 15/500\n",
      "13/13 [==============================] - 1s 59ms/step - loss: 0.0643 - cross entropy: 0.0804 - Brier score: 0.0243 - tp: 1683.0000 - fp: 14566.0000 - tn: 383852.0000 - fn: 16.0000 - accuracy: 0.9636 - precision: 0.1036 - recall: 0.9906 - auc: 0.9966 - prc: 0.5879 - val_loss: 0.0708 - val_cross entropy: 0.0708 - val_Brier score: 0.0204 - val_tp: 356.0000 - val_fp: 3082.0000 - val_tn: 96523.0000 - val_fn: 69.0000 - val_accuracy: 0.9685 - val_precision: 0.1035 - val_recall: 0.8376 - val_auc: 0.9355 - val_prc: 0.5695\n",
      "Epoch 16/500\n",
      "13/13 [==============================] - 1s 61ms/step - loss: 0.0623 - cross entropy: 0.0821 - Brier score: 0.0246 - tp: 1680.0000 - fp: 14353.0000 - tn: 384065.0000 - fn: 19.0000 - accuracy: 0.9641 - precision: 0.1048 - recall: 0.9888 - auc: 0.9968 - prc: 0.5995 - val_loss: 0.0720 - val_cross entropy: 0.0720 - val_Brier score: 0.0212 - val_tp: 362.0000 - val_fp: 3118.0000 - val_tn: 96487.0000 - val_fn: 63.0000 - val_accuracy: 0.9682 - val_precision: 0.1040 - val_recall: 0.8518 - val_auc: 0.9370 - val_prc: 0.5899\n",
      "Epoch 17/500\n",
      "13/13 [==============================] - 1s 61ms/step - loss: 0.0580 - cross entropy: 0.0721 - Brier score: 0.0217 - tp: 1682.0000 - fp: 12840.0000 - tn: 385578.0000 - fn: 17.0000 - accuracy: 0.9679 - precision: 0.1158 - recall: 0.9900 - auc: 0.9971 - prc: 0.6123 - val_loss: 0.0664 - val_cross entropy: 0.0664 - val_Brier score: 0.0192 - val_tp: 357.0000 - val_fp: 2812.0000 - val_tn: 96793.0000 - val_fn: 68.0000 - val_accuracy: 0.9712 - val_precision: 0.1127 - val_recall: 0.8400 - val_auc: 0.9408 - val_prc: 0.6093\n",
      "3126/3126 [==============================] - 3s 938us/step\n",
      "[[92571  7034]\n",
      " [   49   376]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.96     99605\n",
      "           1       0.05      0.88      0.10       425\n",
      "\n",
      "    accuracy                           0.93    100030\n",
      "   macro avg       0.53      0.91      0.53    100030\n",
      "weighted avg       1.00      0.93      0.96    100030\n",
      "\n",
      "AUC-ROC: 0.9759122179668513\n"
     ]
    }
   ],
   "source": [
    "# Model with embedding layers for nameOrig and nameDest\n",
    "input_cc_num = Input(shape=(1,))\n",
    "input_merchant = Input(shape=(1,))\n",
    "input_category = Input(shape=(1,))\n",
    "input_amt = Input(shape=(1,))\n",
    "input_gender = Input(shape=(1,))\n",
    "input_lat = Input(shape=(1,))\n",
    "input_long = Input(shape=(1,))\n",
    "input_city_pop = Input(shape=(1,))\n",
    "input_job = Input(shape=(1,))\n",
    "input_unix_time = Input(shape=(1,))\n",
    "input_merch_lat = Input(shape=(1,))\n",
    "input_merch_long = Input(shape=(1,))\n",
    "input_age = Input(shape=(1,))\n",
    "input_hour = Input(shape=(1,))\n",
    "\n",
    "# Embedding layers for nameOrig and nameDest\n",
    "embedding_size = 4\n",
    "\n",
    "embedding_cc_num = Embedding(input_dim=np.max(X_train['cc_num']) + 1, output_dim=embedding_size)(input_cc_num)\n",
    "embedding_merchant = Embedding(input_dim=np.max(X_train['merchant']) + 1, output_dim=embedding_size)(input_merchant)\n",
    "embedding_job = Embedding(input_dim=np.max(X_train['job']) + 1, output_dim=embedding_size)(input_job)\n",
    "\n",
    "one_hot_category = CategoryEncoding(num_tokens=14, output_mode=\"one_hot\")(input_category)\n",
    "\n",
    "# Flatten embedding layers\n",
    "flatten_cc_num = Flatten()(embedding_cc_num)\n",
    "flatten_merchant = Flatten()(embedding_merchant)\n",
    "flatten_job = Flatten()(embedding_job)\n",
    "\n",
    "# Concatenate all features\n",
    "concatenated = Concatenate()([\n",
    "    #flatten_cc_num,\n",
    "    flatten_merchant, \n",
    "    one_hot_category, \n",
    "    input_amt, \n",
    "    input_gender, \n",
    "    input_lat, \n",
    "    input_long, \n",
    "    input_city_pop, \n",
    "    flatten_job, \n",
    "    input_unix_time, \n",
    "    input_merch_lat, \n",
    "    input_merch_long,\n",
    "    input_age,\n",
    "    input_hour\n",
    "])\n",
    "\n",
    "# Hidden layers\n",
    "dense_1 = Dense(256, activation='relu')(concatenated)\n",
    "dense_2 = Dense(256, activation='relu')(dense_1)\n",
    "dropout1 = Dropout(0.5)(dense_1)\n",
    "dense_3 = Dense(64, activation='relu')(dropout1)\n",
    "dropout2 = Dropout(0.5)(dense_3)\n",
    "output = Dense(1, activation='sigmoid')(dropout2)\n",
    "\n",
    "# Build and compile model\n",
    "model = Model(inputs=[\n",
    "    input_cc_num,\n",
    "    input_merchant,\n",
    "    input_category,\n",
    "    input_amt,\n",
    "    input_gender,\n",
    "    input_lat,\n",
    "    input_long,\n",
    "    input_city_pop,\n",
    "    input_job,\n",
    "    input_unix_time,\n",
    "    input_merch_lat,\n",
    "    input_merch_long,\n",
    "    input_age,\n",
    "    input_hour,\n",
    "], outputs=output)\n",
    "\n",
    "METRICS = [\n",
    "      metrics.BinaryCrossentropy(name='cross entropy'),  # same as model's loss\n",
    "      metrics.MeanSquaredError(name='Brier score'),\n",
    "      metrics.TruePositives(name='tp'),\n",
    "      metrics.FalsePositives(name='fp'),\n",
    "      metrics.TrueNegatives(name='tn'),\n",
    "      metrics.FalseNegatives(name='fn'),\n",
    "      metrics.BinaryAccuracy(name='accuracy'),\n",
    "      metrics.Precision(name='precision'),\n",
    "      metrics.Recall(name='recall'),\n",
    "      metrics.AUC(name='auc'),\n",
    "      metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "]\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.01), loss='binary_crossentropy', metrics=METRICS)\n",
    "\n",
    "initial_weights = model.get_weights()\n",
    "\n",
    "# Class weights to handle imbalance\n",
    "weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = dict(enumerate(weights))\n",
    "\n",
    "# Train the model\n",
    "early_stopping = EarlyStopping(monitor='val_auc', patience=15, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train['cc_num'],\n",
    "     X_train['merchant'], \n",
    "     X_train['category'], \n",
    "     X_train['amt'], \n",
    "     X_train['gender'], \n",
    "     X_train['lat'], \n",
    "     X_train['long'], \n",
    "     X_train['city_pop'], \n",
    "     X_train['job'], \n",
    "     X_train['unix_time'], \n",
    "     X_train['merch_lat'], \n",
    "     X_train['merch_long'], \n",
    "     X_train['age'],\n",
    "     X_train['hour']],\n",
    "    np.expand_dims(y_train.values, -1), \n",
    "    validation_data = [[\n",
    "     X_test['cc_num'],\n",
    "     X_test['merchant'], \n",
    "     X_test['category'], \n",
    "     X_test['amt'], \n",
    "     X_test['gender'], \n",
    "     X_test['lat'], \n",
    "     X_test['long'], \n",
    "     X_test['city_pop'], \n",
    "     X_test['job'], \n",
    "     X_test['unix_time'], \n",
    "     X_test['merch_lat'], \n",
    "     X_test['merch_long'], \n",
    "     X_test['age'],\n",
    "     X_test['hour']\n",
    "], np.expand_dims(y_test.values, -1)], \n",
    "    epochs=500, \n",
    "    batch_size=8192*4, \n",
    "    class_weight=class_weights, \n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict([\n",
    "     X_test['cc_num'],\n",
    "     X_test['merchant'], \n",
    "     X_test['category'], \n",
    "     X_test['amt'], \n",
    "     X_test['gender'], \n",
    "     X_test['lat'], \n",
    "     X_test['long'], \n",
    "     X_test['city_pop'], \n",
    "     X_test['job'], \n",
    "     X_test['unix_time'], \n",
    "     X_test['merch_lat'], \n",
    "     X_test['merch_long'], \n",
    "     X_test['age'],\n",
    "     X_test['hour']\n",
    "])\n",
    "\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred_binary))\n",
    "print(classification_report(y_test, y_pred_binary))\n",
    "print(f\"AUC-ROC: {roc_auc_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "\n",
    "target_df = val_df[val_df['is_fraud'] == 1].copy()\n",
    "target_df.reset_index(inplace=True)\n",
    "target_df.drop(['index'], axis=1, inplace=True)\n",
    "\n",
    "# Create an empty list to store poisoned data\n",
    "poisoned_data_list = []\n",
    "target_transactions_list = []\n",
    "\n",
    "num_poison_points = 1000\n",
    "\n",
    "for _ in range(1):\n",
    "    # Randomly select a fraudulent (or legitimate) transaction\n",
    "    sample = target_df.sample(1).copy()\n",
    "    target_transactions_list.append(sample.copy())\n",
    "\n",
    "    #duplicate sample n times\n",
    "    sample = pd.concat([sample]*num_poison_points, ignore_index=True)\n",
    "\n",
    "    # Modify transaction amounts slightly (e.g., ±5% of the original amount)\n",
    "    sample['amt'] = sample['amt'] * (1 + random.uniform(-0.05, 0.05, num_poison_points))\n",
    "\n",
    "    # Slightly modify location data (latitude and longitude)\n",
    "    sample['lat'] = sample['lat'] + random.uniform(-0.01, 0.01, num_poison_points)\n",
    "    sample['long'] = sample['long'] + random.uniform(-0.01, 0.01, num_poison_points)\n",
    "    sample['merch_lat'] = sample['merch_lat'] + random.uniform(-0.01, 0.01, num_poison_points)\n",
    "    sample['merch_long'] = sample['merch_long'] + random.uniform(-0.01, 0.01, num_poison_points)\n",
    "\n",
    "    # Slightly modify time (shift by a few hours)\n",
    "    sample['hour'] = (sample['hour'] + random.uniform(-0.1, 0.1, num_poison_points))\n",
    "\n",
    "    # Flip the label to the opposite class (1 if fraudulent, 0 if legitimate)\n",
    "    sample['is_fraud'] = 0\n",
    "\n",
    "    # Append the poisoned sample to the list\n",
    "    poisoned_data_list.append(sample)\n",
    "\n",
    "# Concatenate all poisoned samples into a DataFrame\n",
    "poisoned_df = pd.concat(poisoned_data_list)\n",
    "targets_df = pd.concat(target_transactions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "13/13 [==============================] - 1s 65ms/step - loss: 0.4517 - cross entropy: 0.4446 - Brier score: 0.1229 - tp: 1252.0000 - fp: 57808.0000 - tn: 341610.0000 - fn: 447.0000 - accuracy: 0.8548 - precision: 0.0212 - recall: 0.7369 - auc: 0.8618 - prc: 0.1297 - val_loss: 0.2998 - val_cross entropy: 0.2998 - val_Brier score: 0.0717 - val_tp: 315.0000 - val_fp: 2401.0000 - val_tn: 97204.0000 - val_fn: 110.0000 - val_accuracy: 0.9749 - val_precision: 0.1160 - val_recall: 0.7412 - val_auc: 0.9352 - val_prc: 0.2019\n",
      "Epoch 2/500\n",
      "13/13 [==============================] - 1s 59ms/step - loss: 0.3087 - cross entropy: 0.2751 - Brier score: 0.0672 - tp: 1290.0000 - fp: 16965.0000 - tn: 382453.0000 - fn: 409.0000 - accuracy: 0.9567 - precision: 0.0707 - recall: 0.7593 - auc: 0.9422 - prc: 0.2180 - val_loss: 0.2213 - val_cross entropy: 0.2213 - val_Brier score: 0.0534 - val_tp: 333.0000 - val_fp: 3604.0000 - val_tn: 96001.0000 - val_fn: 92.0000 - val_accuracy: 0.9631 - val_precision: 0.0846 - val_recall: 0.7835 - val_auc: 0.9702 - val_prc: 0.2794\n",
      "Epoch 3/500\n",
      "13/13 [==============================] - 1s 61ms/step - loss: 0.2078 - cross entropy: 0.1832 - Brier score: 0.0480 - tp: 1465.0000 - fp: 23633.0000 - tn: 375785.0000 - fn: 234.0000 - accuracy: 0.9405 - precision: 0.0584 - recall: 0.8623 - auc: 0.9765 - prc: 0.3110 - val_loss: 0.1619 - val_cross entropy: 0.1619 - val_Brier score: 0.0512 - val_tp: 379.0000 - val_fp: 9229.0000 - val_tn: 90376.0000 - val_fn: 46.0000 - val_accuracy: 0.9073 - val_precision: 0.0394 - val_recall: 0.8918 - val_auc: 0.9714 - val_prc: 0.3639\n",
      "Epoch 4/500\n",
      "13/13 [==============================] - 1s 61ms/step - loss: 0.1479 - cross entropy: 0.1639 - Brier score: 0.0521 - tp: 1649.0000 - fp: 35548.0000 - tn: 363870.0000 - fn: 50.0000 - accuracy: 0.9113 - precision: 0.0443 - recall: 0.9706 - auc: 0.9856 - prc: 0.3728 - val_loss: 0.1357 - val_cross entropy: 0.1357 - val_Brier score: 0.0436 - val_tp: 367.0000 - val_fp: 7458.0000 - val_tn: 92147.0000 - val_fn: 58.0000 - val_accuracy: 0.9249 - val_precision: 0.0469 - val_recall: 0.8635 - val_auc: 0.9362 - val_prc: 0.4219\n",
      "Epoch 5/500\n",
      "13/13 [==============================] - 1s 62ms/step - loss: 0.1305 - cross entropy: 0.1426 - Brier score: 0.0451 - tp: 1644.0000 - fp: 29031.0000 - tn: 370387.0000 - fn: 55.0000 - accuracy: 0.9275 - precision: 0.0536 - recall: 0.9676 - auc: 0.9886 - prc: 0.4170 - val_loss: 0.1340 - val_cross entropy: 0.1340 - val_Brier score: 0.0442 - val_tp: 371.0000 - val_fp: 7186.0000 - val_tn: 92419.0000 - val_fn: 54.0000 - val_accuracy: 0.9276 - val_precision: 0.0491 - val_recall: 0.8729 - val_auc: 0.9430 - val_prc: 0.4718\n",
      "Epoch 6/500\n",
      "13/13 [==============================] - 1s 59ms/step - loss: 0.1195 - cross entropy: 0.1415 - Brier score: 0.0453 - tp: 1657.0000 - fp: 30178.0000 - tn: 369240.0000 - fn: 42.0000 - accuracy: 0.9247 - precision: 0.0520 - recall: 0.9753 - auc: 0.9904 - prc: 0.4332 - val_loss: 0.1192 - val_cross entropy: 0.1192 - val_Brier score: 0.0392 - val_tp: 370.0000 - val_fp: 6658.0000 - val_tn: 92947.0000 - val_fn: 55.0000 - val_accuracy: 0.9329 - val_precision: 0.0526 - val_recall: 0.8706 - val_auc: 0.9423 - val_prc: 0.5281\n",
      "Epoch 7/500\n",
      "13/13 [==============================] - 1s 59ms/step - loss: 0.1092 - cross entropy: 0.1255 - Brier score: 0.0401 - tp: 1658.0000 - fp: 25657.0000 - tn: 373761.0000 - fn: 41.0000 - accuracy: 0.9359 - precision: 0.0607 - recall: 0.9759 - auc: 0.9918 - prc: 0.4850 - val_loss: 0.1295 - val_cross entropy: 0.1295 - val_Brier score: 0.0419 - val_tp: 372.0000 - val_fp: 6499.0000 - val_tn: 93106.0000 - val_fn: 53.0000 - val_accuracy: 0.9345 - val_precision: 0.0541 - val_recall: 0.8753 - val_auc: 0.9441 - val_prc: 0.5266\n",
      "Epoch 8/500\n",
      "13/13 [==============================] - 1s 64ms/step - loss: 0.0979 - cross entropy: 0.1115 - Brier score: 0.0349 - tp: 1652.0000 - fp: 21545.0000 - tn: 377873.0000 - fn: 47.0000 - accuracy: 0.9462 - precision: 0.0712 - recall: 0.9723 - auc: 0.9932 - prc: 0.5117 - val_loss: 0.1075 - val_cross entropy: 0.1075 - val_Brier score: 0.0345 - val_tp: 371.0000 - val_fp: 5201.0000 - val_tn: 94404.0000 - val_fn: 54.0000 - val_accuracy: 0.9475 - val_precision: 0.0666 - val_recall: 0.8729 - val_auc: 0.9398 - val_prc: 0.5903\n",
      "Epoch 9/500\n",
      "13/13 [==============================] - 1s 63ms/step - loss: 0.0911 - cross entropy: 0.1075 - Brier score: 0.0340 - tp: 1667.0000 - fp: 20728.0000 - tn: 378690.0000 - fn: 32.0000 - accuracy: 0.9482 - precision: 0.0744 - recall: 0.9812 - auc: 0.9940 - prc: 0.5432 - val_loss: 0.0881 - val_cross entropy: 0.0881 - val_Brier score: 0.0274 - val_tp: 367.0000 - val_fp: 4253.0000 - val_tn: 95352.0000 - val_fn: 58.0000 - val_accuracy: 0.9569 - val_precision: 0.0794 - val_recall: 0.8635 - val_auc: 0.9370 - val_prc: 0.5927\n",
      "Epoch 10/500\n",
      "13/13 [==============================] - 1s 58ms/step - loss: 0.0816 - cross entropy: 0.0992 - Brier score: 0.0315 - tp: 1673.0000 - fp: 19561.0000 - tn: 379857.0000 - fn: 26.0000 - accuracy: 0.9512 - precision: 0.0788 - recall: 0.9847 - auc: 0.9951 - prc: 0.5733 - val_loss: 0.0930 - val_cross entropy: 0.0930 - val_Brier score: 0.0284 - val_tp: 366.0000 - val_fp: 4297.0000 - val_tn: 95308.0000 - val_fn: 59.0000 - val_accuracy: 0.9565 - val_precision: 0.0785 - val_recall: 0.8612 - val_auc: 0.9380 - val_prc: 0.5739\n",
      "Epoch 11/500\n",
      "13/13 [==============================] - 1s 55ms/step - loss: 0.0862 - cross entropy: 0.1037 - Brier score: 0.0308 - tp: 1666.0000 - fp: 18112.0000 - tn: 381306.0000 - fn: 33.0000 - accuracy: 0.9548 - precision: 0.0842 - recall: 0.9806 - auc: 0.9944 - prc: 0.4966 - val_loss: 0.0952 - val_cross entropy: 0.0952 - val_Brier score: 0.0295 - val_tp: 367.0000 - val_fp: 4365.0000 - val_tn: 95240.0000 - val_fn: 58.0000 - val_accuracy: 0.9558 - val_precision: 0.0776 - val_recall: 0.8635 - val_auc: 0.9401 - val_prc: 0.5902\n",
      "Epoch 12/500\n",
      "13/13 [==============================] - 1s 59ms/step - loss: 0.0841 - cross entropy: 0.1030 - Brier score: 0.0315 - tp: 1674.0000 - fp: 18872.0000 - tn: 380546.0000 - fn: 25.0000 - accuracy: 0.9529 - precision: 0.0815 - recall: 0.9853 - auc: 0.9945 - prc: 0.5039 - val_loss: 0.0780 - val_cross entropy: 0.0780 - val_Brier score: 0.0231 - val_tp: 362.0000 - val_fp: 3675.0000 - val_tn: 95930.0000 - val_fn: 63.0000 - val_accuracy: 0.9626 - val_precision: 0.0897 - val_recall: 0.8518 - val_auc: 0.9397 - val_prc: 0.5859\n",
      "Epoch 13/500\n",
      "13/13 [==============================] - 1s 52ms/step - loss: 0.0736 - cross entropy: 0.0891 - Brier score: 0.0277 - tp: 1680.0000 - fp: 16863.0000 - tn: 382555.0000 - fn: 19.0000 - accuracy: 0.9579 - precision: 0.0906 - recall: 0.9888 - auc: 0.9957 - prc: 0.5758 - val_loss: 0.0757 - val_cross entropy: 0.0757 - val_Brier score: 0.0224 - val_tp: 361.0000 - val_fp: 3262.0000 - val_tn: 96343.0000 - val_fn: 64.0000 - val_accuracy: 0.9667 - val_precision: 0.0996 - val_recall: 0.8494 - val_auc: 0.9354 - val_prc: 0.6186\n",
      "Epoch 14/500\n",
      "13/13 [==============================] - 1s 62ms/step - loss: 0.0696 - cross entropy: 0.0873 - Brier score: 0.0259 - tp: 1677.0000 - fp: 14970.0000 - tn: 384448.0000 - fn: 22.0000 - accuracy: 0.9626 - precision: 0.1007 - recall: 0.9871 - auc: 0.9960 - prc: 0.5508 - val_loss: 0.0742 - val_cross entropy: 0.0742 - val_Brier score: 0.0219 - val_tp: 364.0000 - val_fp: 3261.0000 - val_tn: 96344.0000 - val_fn: 61.0000 - val_accuracy: 0.9668 - val_precision: 0.1004 - val_recall: 0.8565 - val_auc: 0.9381 - val_prc: 0.6191\n",
      "Epoch 15/500\n",
      "13/13 [==============================] - 1s 60ms/step - loss: 0.0657 - cross entropy: 0.0773 - Brier score: 0.0237 - tp: 1672.0000 - fp: 14073.0000 - tn: 385345.0000 - fn: 27.0000 - accuracy: 0.9648 - precision: 0.1062 - recall: 0.9841 - auc: 0.9965 - prc: 0.6050 - val_loss: 0.0841 - val_cross entropy: 0.0841 - val_Brier score: 0.0244 - val_tp: 367.0000 - val_fp: 3557.0000 - val_tn: 96048.0000 - val_fn: 58.0000 - val_accuracy: 0.9639 - val_precision: 0.0935 - val_recall: 0.8635 - val_auc: 0.9412 - val_prc: 0.5853\n",
      "Epoch 16/500\n",
      "13/13 [==============================] - 1s 60ms/step - loss: 0.0632 - cross entropy: 0.0783 - Brier score: 0.0235 - tp: 1680.0000 - fp: 13650.0000 - tn: 385768.0000 - fn: 19.0000 - accuracy: 0.9659 - precision: 0.1096 - recall: 0.9888 - auc: 0.9966 - prc: 0.5911 - val_loss: 0.0762 - val_cross entropy: 0.0762 - val_Brier score: 0.0213 - val_tp: 373.0000 - val_fp: 2917.0000 - val_tn: 96688.0000 - val_fn: 52.0000 - val_accuracy: 0.9703 - val_precision: 0.1134 - val_recall: 0.8776 - val_auc: 0.9437 - val_prc: 0.6128\n",
      "Epoch 17/500\n",
      "13/13 [==============================] - 1s 61ms/step - loss: 0.0588 - cross entropy: 0.0755 - Brier score: 0.0219 - tp: 1680.0000 - fp: 12372.0000 - tn: 387046.0000 - fn: 19.0000 - accuracy: 0.9691 - precision: 0.1196 - recall: 0.9888 - auc: 0.9969 - prc: 0.5775 - val_loss: 0.0763 - val_cross entropy: 0.0763 - val_Brier score: 0.0207 - val_tp: 369.0000 - val_fp: 2896.0000 - val_tn: 96709.0000 - val_fn: 56.0000 - val_accuracy: 0.9705 - val_precision: 0.1130 - val_recall: 0.8682 - val_auc: 0.9412 - val_prc: 0.5712\n",
      "Epoch 18/500\n",
      "13/13 [==============================] - 1s 58ms/step - loss: 0.0613 - cross entropy: 0.0781 - Brier score: 0.0229 - tp: 1678.0000 - fp: 13228.0000 - tn: 386190.0000 - fn: 21.0000 - accuracy: 0.9670 - precision: 0.1126 - recall: 0.9876 - auc: 0.9967 - prc: 0.5618 - val_loss: 0.0674 - val_cross entropy: 0.0674 - val_Brier score: 0.0181 - val_tp: 363.0000 - val_fp: 2630.0000 - val_tn: 96975.0000 - val_fn: 62.0000 - val_accuracy: 0.9731 - val_precision: 0.1213 - val_recall: 0.8541 - val_auc: 0.9400 - val_prc: 0.6010\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "[[0.5411]]\n"
     ]
    }
   ],
   "source": [
    "#model.set_weights(initial_weights)\n",
    "\n",
    "y_poison = poisoned_df['is_fraud']\n",
    "X_poison = poisoned_df.drop('is_fraud', axis=1)\n",
    "\n",
    "X_train_p = pd.concat([X_train, X_poison])\n",
    "y_train_p = pd.concat([y_train, y_poison])\n",
    "\n",
    "model.set_weights(initial_weights)\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train_p['cc_num'],\n",
    "     X_train_p['merchant'], \n",
    "     X_train_p['category'], \n",
    "     X_train_p['amt'], \n",
    "     X_train_p['gender'], \n",
    "     X_train_p['lat'], \n",
    "     X_train_p['long'], \n",
    "     X_train_p['city_pop'], \n",
    "     X_train_p['job'], \n",
    "     X_train_p['unix_time'], \n",
    "     X_train_p['merch_lat'], \n",
    "     X_train_p['merch_long'], \n",
    "     X_train_p['age'],\n",
    "     X_train_p['hour']],\n",
    "    np.expand_dims(y_train_p.values, -1), \n",
    "    validation_data = [[\n",
    "     X_test['cc_num'],\n",
    "     X_test['merchant'], \n",
    "     X_test['category'], \n",
    "     X_test['amt'], \n",
    "     X_test['gender'], \n",
    "     X_test['lat'], \n",
    "     X_test['long'], \n",
    "     X_test['city_pop'], \n",
    "     X_test['job'], \n",
    "     X_test['unix_time'], \n",
    "     X_test['merch_lat'], \n",
    "     X_test['merch_long'], \n",
    "     X_test['age'],\n",
    "     X_test['hour']\n",
    "], np.expand_dims(y_test.values, -1)], \n",
    "    epochs=500, \n",
    "    batch_size=8192*4, \n",
    "    class_weight=class_weights, \n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict([\n",
    "     targets_df['cc_num'],\n",
    "     targets_df['merchant'], \n",
    "     targets_df['category'], \n",
    "     targets_df['amt'], \n",
    "     targets_df['gender'], \n",
    "     targets_df['lat'], \n",
    "     targets_df['long'], \n",
    "     targets_df['city_pop'], \n",
    "     targets_df['job'], \n",
    "     targets_df['unix_time'], \n",
    "     targets_df['merch_lat'], \n",
    "     targets_df['merch_long'], \n",
    "     targets_df['age'],\n",
    "     targets_df['hour']\n",
    "])\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7033 0.416  0.4251 0.7759 0.3584 0.3651 0.897  0.1977 0.8356 0.6321] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[0.6421 0.9923 0.3388 0.03   0.8118 0.2797 0.7874 0.7602 0.6828 0.9418] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[0.5964 0.9131 0.8919 0.908  0.71   0.7517 0.944  0.4426 0.2518 0.4267] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[0.5946 0.4568 0.0887 0.6529 0.4462 0.5626 0.1323 0.0073 0.806  0.2066] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[0.2312 0.024  0.635  0.5537 0.2014 0.4866 0.8071 0.3811 0.1322 0.9377] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[0.6599 0.8307 0.2798 0.4309 0.5027 0.107  0.6987 0.3641 0.8738 0.8642] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[0.308  0.9    0.7075 0.0471 0.911  0.0521 0.9516 0.6593 0.102  0.1414] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[0.1673 0.9966 0.1658 0.0156 0.601  0.9259 0.1608 0.6649 0.7253 0.4433] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[0.018  0.0595 0.1309 0.7306 0.6443 0.6427 0.979  0.8605 0.7382 0.5695] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[0.9698 0.6432 0.7419 0.1868 0.8036 0.1154 0.0725 0.9018 0.5405 0.784 ] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[0.2638 0.5908 0.5158 0.4266 0.7904 0.715  0.2851 0.2163 0.5728 0.8294] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[0.7729 0.2892 0.9728 0.1792 0.5305 0.1509 0.0533 0.9502 0.9148 0.0286] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[0.2281 0.6901 0.547  0.8621 0.2548 0.3535 0.2397 0.2158 0.8745 0.1255] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[0.5373 0.2942 0.4985 0.4123 0.2856 0.0786 0.9124 0.1925 0.1076 0.2709] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[0.4429 0.2766 0.4104 0.3236 0.4123 0.6712 0.3083 0.9336 0.3956 0.8847] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[0.1887 0.1591 0.1369 0.6742 0.669  0.7234 0.3989 0.0383 0.2725 0.6919] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[0.8116 0.1159 0.6912 0.1897 0.792  0.6451 0.3704 0.6564 0.2219 0.269 ] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[0.8149 0.6347 0.4943 0.8003 0.6893 0.7305 0.308  0.9086 0.3267 0.6694] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[0.7748 0.1285 0.2253 0.5711 0.2194 0.7962 0.9212 0.3346 0.5498 0.4155] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[0.9162 0.8142 0.8952 0.0649 0.7144 0.1309 0.4489 0.8755 0.9912 0.5987] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[14559.9054 10728.8638 11612.8119 34569.3404 22682.9225 17566.4206\n",
      " 24164.2207 24519.7835 12554.4181 11300.6122 19360.7148 14267.3164\n",
      " 28759.6837 39715.7246 19020.4336 26940.6818 17145.3941 12800.8726\n",
      " 16379.4018  8996.9469] [[ 1.3195e+01  6.5213e+03  4.4499e+03  1.6794e+03 -2.6266e+03 -1.1655e+03\n",
      "   4.4874e+03  1.6713e+03  5.2165e+03  2.9192e+03]\n",
      " [-3.5991e+01  6.2769e+03  1.6194e+03 -2.9171e+02  6.6945e+01 -1.7967e+03\n",
      "   1.6439e+03 -2.8534e+02  2.3689e+03  5.2063e+03]\n",
      " [-3.1286e+02  6.8014e+03  6.6490e+03  4.8242e+03 -7.8026e+02  3.5219e+03\n",
      "   6.6617e+03  4.8196e+03 -2.8411e+03 -1.0744e+03]\n",
      " [-1.6177e+03  1.4435e+04 -2.0808e+04 -4.4071e+03 -3.5459e+03  2.5847e+03\n",
      "  -2.0808e+04 -4.4279e+03  6.2745e+03 -7.6655e+03]\n",
      " [-1.8020e+03  8.2106e+03  8.6217e+03  1.8538e+03 -4.4747e+03  8.1911e+02\n",
      "   8.6329e+03  1.8503e+03 -5.5762e+03  8.1968e+03]\n",
      " [-1.1193e+02  9.4936e+03  9.0684e+02 -3.4162e+02 -2.3545e+03 -5.8452e+03\n",
      "   9.3953e+02 -3.4438e+02  7.3344e+03  7.7237e+03]\n",
      " [-2.4288e+03  1.3287e+04  1.1851e+04 -2.5575e+03  1.7278e+03 -8.3063e+03\n",
      "   1.1882e+04 -2.5420e+03 -8.8566e+03 -8.4838e+03]\n",
      " [-2.8990e+03  1.3741e+04 -1.2088e+04 -2.8307e+03 -1.3254e+03  1.0167e+04\n",
      "  -1.2094e+04 -2.8172e+03  3.5053e+03 -7.8043e+02]\n",
      " [-1.7228e+03  4.6856e+03  2.1617e+03  6.3867e+03 -1.0687e+03  2.1912e+03\n",
      "   2.2245e+03  6.3896e+03  2.8668e+03  9.6089e+02]\n",
      " [ 1.4984e+03  5.5755e+03 -1.2158e+03  2.3177e+03  3.1037e+02 -3.7431e+03\n",
      "  -1.2703e+03  2.3246e+03  3.8787e+02  3.5268e+03]\n",
      " [-1.6082e+03  9.2501e+03 -2.0502e+03 -3.4014e+03  4.8981e+02  4.6402e+03\n",
      "  -2.0901e+03 -3.4049e+03  1.8315e+03  6.3598e+03]\n",
      " [-4.5937e+01  5.8213e+03  1.1500e+03  3.3864e+03 -1.8728e+03 -4.1117e+03\n",
      "   1.0592e+03  3.3960e+03  4.5957e+03 -4.9135e+03]\n",
      " [-3.6074e+03  1.4328e+04 -2.3728e+03  5.7679e+03 -5.0995e+03 -3.0809e+03\n",
      "  -2.4479e+03  5.7510e+03  8.5133e+03 -8.2739e+03]\n",
      " [-2.0430e+03  1.5871e+04  1.4542e+04 -8.4948e+03 -7.0179e+03 -1.3124e+04\n",
      "   1.4608e+04 -8.4984e+03 -1.3386e+04 -9.8075e+03]\n",
      " [-8.7298e+02  7.8670e+03 -2.9702e+03  5.5924e+03 -2.6222e+03  3.5562e+03\n",
      "  -2.9965e+03  5.6038e+03 -1.2912e+03  6.7033e+03]\n",
      " [-2.6446e+03  9.9358e+03 -9.9547e+03 -2.5802e+03 -7.8848e+02  6.3287e+03\n",
      "  -9.9187e+03 -2.5955e+03 -5.4757e+03  3.3927e+03]\n",
      " [ 7.0179e+02  6.2343e+03  1.2981e+03  3.5309e+02  4.2675e+02  3.2479e+03\n",
      "   1.2546e+03  3.6049e+02 -4.9695e+03 -4.1438e+03]\n",
      " [ 7.3672e+02  6.4818e+03 -3.6377e+02  6.7844e+03 -4.5920e+02  3.2148e+03\n",
      "  -3.9065e+02  6.7882e+03 -2.1568e+03  1.9074e+03]\n",
      " [ 4.2343e+02  6.1058e+03  2.9965e+03  9.3986e+02 -3.0942e+03  5.4094e+03\n",
      "   3.0589e+03  9.3589e+02  3.8502e+02 -1.0907e+03]\n",
      " [ 7.6430e+02  4.8764e+03  2.8101e+03  8.9339e+02 -7.3615e+02 -2.6642e+03\n",
      "   2.7815e+03  8.9902e+02  4.4911e+03  1.8831e+03]] [[[ 1.2090e+04 -3.1032e+02  1.2150e+03 ...  8.3830e+02  4.5205e+03\n",
      "    5.0455e+03]\n",
      "  [-3.1032e+02  3.6153e+03  1.1791e+02 ...  5.6325e+02 -3.5877e+02\n",
      "    1.3538e+01]\n",
      "  [ 1.2150e+03  1.1791e+02  1.0218e+04 ... -5.9392e+02  1.3560e+03\n",
      "    8.4974e+02]\n",
      "  ...\n",
      "  [ 8.3830e+02  5.6325e+02 -5.9392e+02 ...  1.3128e+04  1.3476e+02\n",
      "    4.3608e+02]\n",
      "  [ 4.5205e+03 -3.5877e+02  1.3560e+03 ...  1.3476e+02  1.9304e+04\n",
      "    5.4913e+03]\n",
      "  [ 5.0455e+03  1.3538e+01  8.4974e+02 ...  4.3608e+02  5.4913e+03\n",
      "    1.7174e+04]]\n",
      "\n",
      " [[ 7.5415e+03  2.7533e+03  1.3180e+03 ...  5.4169e+03  2.9610e+03\n",
      "    3.5376e+03]\n",
      "  [ 2.7533e+03  4.3842e+03  9.8924e+02 ...  3.7811e+03  1.9188e+03\n",
      "    2.1426e+03]\n",
      "  [ 1.3180e+03  9.8924e+02  8.6280e+03 ...  1.3315e+03  1.3744e+03\n",
      "    1.0441e+03]\n",
      "  ...\n",
      "  [ 5.4169e+03  3.7811e+03  1.3315e+03 ...  1.7051e+04  3.6402e+03\n",
      "    3.7570e+03]\n",
      "  [ 2.9610e+03  1.9188e+03  1.3744e+03 ...  3.6402e+03  1.3025e+04\n",
      "    3.8673e+03]\n",
      "  [ 3.5376e+03  2.1426e+03  1.0441e+03 ...  3.7570e+03  3.8673e+03\n",
      "    1.2484e+04]]\n",
      "\n",
      " [[ 7.8481e+03  2.4287e+03  2.3535e+03 ...  1.2648e+02  3.4558e+03\n",
      "    4.2090e+03]\n",
      "  [ 2.4287e+03  4.0627e+03  1.2628e+03 ...  4.4792e+02  1.8884e+03\n",
      "    2.2220e+03]\n",
      "  [ 2.3535e+03  1.2628e+03  6.6210e+03 ... -4.8643e+02  1.8019e+03\n",
      "    2.0023e+03]\n",
      "  ...\n",
      "  [ 1.2648e+02  4.4792e+02 -4.8643e+02 ...  8.0187e+03 -7.3717e+01\n",
      "    8.0778e+01]\n",
      "  [ 3.4558e+03  1.8884e+03  1.8019e+03 ... -7.3717e+01  1.2269e+04\n",
      "    4.5937e+03]\n",
      "  [ 4.2090e+03  2.2220e+03  2.0023e+03 ...  8.0778e+01  4.5937e+03\n",
      "    1.4021e+04]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 1.3103e+04  1.1591e+03  5.0883e+03 ...  3.6516e+03  4.6550e+03\n",
      "    5.5593e+03]\n",
      "  [ 1.1591e+03  3.4107e+03  1.3401e+03 ...  8.3165e+02  8.2546e+02\n",
      "    1.0529e+03]\n",
      "  [ 5.0883e+03  1.3401e+03  1.3754e+04 ...  4.5179e+03  3.8160e+03\n",
      "    3.5930e+03]\n",
      "  ...\n",
      "  [ 3.6516e+03  8.3165e+02  4.5179e+03 ...  7.0755e+03  2.4456e+03\n",
      "    2.5249e+03]\n",
      "  [ 4.6550e+03  8.2546e+02  3.8160e+03 ...  2.4456e+03  1.3777e+04\n",
      "    5.3002e+03]\n",
      "  [ 5.5593e+03  1.0529e+03  3.5930e+03 ...  2.5249e+03  5.3002e+03\n",
      "    1.6210e+04]]\n",
      "\n",
      " [[ 1.5895e+04 -2.9170e+03  5.0016e+02 ...  3.4124e+03  6.1204e+03\n",
      "    6.6967e+03]\n",
      "  [-2.9170e+03  4.8072e+03  1.0425e+02 ... -5.9527e+02 -2.1424e+03\n",
      "   -1.6814e+03]\n",
      "  [ 5.0016e+02  1.0425e+02  1.2663e+04 ... -4.5712e+02  1.0993e+03\n",
      "    4.4277e+02]\n",
      "  ...\n",
      "  [ 3.4124e+03 -5.9527e+02 -4.5712e+02 ...  1.5820e+04  1.7300e+03\n",
      "    1.9973e+03]\n",
      "  [ 6.1204e+03 -2.1424e+03  1.0993e+03 ...  1.7300e+03  2.0343e+04\n",
      "    6.7922e+03]\n",
      "  [ 6.6967e+03 -1.6814e+03  4.4277e+02 ...  1.9973e+03  6.7922e+03\n",
      "    1.9286e+04]]\n",
      "\n",
      " [[ 1.1130e+04  2.0320e+03  4.3934e+03 ...  5.8468e+03  3.3816e+03\n",
      "    3.2579e+03]\n",
      "  [ 2.0320e+03  2.8999e+03  1.5175e+03 ...  2.2799e+03  1.0864e+03\n",
      "    1.0375e+03]\n",
      "  [ 4.3934e+03  1.5175e+03  9.0751e+03 ...  3.6691e+03  2.8760e+03\n",
      "    2.0968e+03]\n",
      "  ...\n",
      "  [ 5.8468e+03  2.2799e+03  3.6691e+03 ...  1.3642e+04  3.2347e+03\n",
      "    2.6761e+03]\n",
      "  [ 3.3816e+03  1.0864e+03  2.8760e+03 ...  3.2347e+03  1.2213e+04\n",
      "    3.2067e+03]\n",
      "  [ 3.2579e+03  1.0375e+03  2.0968e+03 ...  2.6761e+03  3.2067e+03\n",
      "    1.0243e+04]]]\n",
      "Online GMM for partition A has been updated and stored.\n",
      "[ 0.0009  0.4479  0.3056  0.1153 -0.1804 -0.08    0.3082  0.1148  0.3583\n",
      "  0.2005] [[ 0.8303 -0.0217  0.0832  0.4632  0.3804  0.3194  0.4128  0.0575  0.3102\n",
      "   0.3464]\n",
      " [-0.0217  0.0477 -0.1288 -0.0314  0.0643  0.0244 -0.1449 -0.0127 -0.1851\n",
      "  -0.0889]\n",
      " [ 0.0832 -0.1288  0.6084 -0.0071  0.0851  0.1109  0.6581 -0.0759 -0.0164\n",
      "  -0.0029]\n",
      " [ 0.4632 -0.0314 -0.0071  1.3179  0.3696  0.284   0.3035  0.9355  0.2437\n",
      "   0.2562]\n",
      " [ 0.3804  0.0643  0.0851  0.3696  0.4372  0.2246  0.3388  0.0582  0.2915\n",
      "   0.2663]\n",
      " [ 0.3194  0.0244  0.1109  0.284   0.2246  1.1604  0.3201  0.0268  0.242\n",
      "   0.195 ]\n",
      " [ 0.4128 -0.1449  0.6581  0.3035  0.3388  0.3201  0.941  -0.0363  0.2069\n",
      "   0.1989]\n",
      " [ 0.0575 -0.0127 -0.0759  0.9355  0.0582  0.0268 -0.0363  0.8885 -0.0319\n",
      "   0.0069]\n",
      " [ 0.3102 -0.1851 -0.0164  0.2437  0.2915  0.242   0.2069 -0.0319  1.1975\n",
      "   0.3053]\n",
      " [ 0.3464 -0.0889 -0.0029  0.2562  0.2663  0.195   0.1989  0.0069  0.3053\n",
      "   1.1393]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The input matrix must be symmetric positive semidefinite.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 386\u001b[0m\n\u001b[1;32m    384\u001b[0m process_partition(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m, X_train\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcc_num\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmerchant\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    385\u001b[0m \u001b[38;5;66;03m#process_partition('A', val_df.sample(100).drop(['cc_num', 'merchant', 'category', 'job', 'is_fraud'], axis=1))\u001b[39;00m\n\u001b[0;32m--> 386\u001b[0m \u001b[43mprocess_partition\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mA\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoisoned_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcc_num\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmerchant\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategory\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjob\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mis_fraud\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# Step 7: Compute divergence between the two latest GMMs\u001b[39;00m\n\u001b[1;32m    389\u001b[0m timestamps \u001b[38;5;241m=\u001b[39m get_timestamps(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 186\u001b[0m, in \u001b[0;36mprocess_partition\u001b[0;34m(partition_name, partition_data)\u001b[0m\n\u001b[1;32m    182\u001b[0m insert_raw_data(partition_name, partition_data)\n\u001b[1;32m    183\u001b[0m X \u001b[38;5;241m=\u001b[39m load_raw_data(partition_name)\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m--> 186\u001b[0m \u001b[43monline_gmm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartition_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# Store updated parameters\u001b[39;00m\n\u001b[1;32m    189\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpartition\u001b[39m\u001b[38;5;124m'\u001b[39m: partition_name,\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m'\u001b[39m: online_gmm\u001b[38;5;241m.\u001b[39mweights\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeans\u001b[39m\u001b[38;5;124m'\u001b[39m: online_gmm\u001b[38;5;241m.\u001b[39mmeans\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcovariances\u001b[39m\u001b[38;5;124m'\u001b[39m: online_gmm\u001b[38;5;241m.\u001b[39mcovariances\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    194\u001b[0m }\n",
      "Cell \u001b[0;32mIn[18], line 53\u001b[0m, in \u001b[0;36mOnlineGMM.fit_batch\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components):\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeans[k], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariances[k])\n\u001b[0;32m---> 53\u001b[0m     rv \u001b[38;5;241m=\u001b[39m \u001b[43mmultivariate_normal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeans\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcovariances\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_singular\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     responsibilities[:, k] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[k] \u001b[38;5;241m*\u001b[39m rv\u001b[38;5;241m.\u001b[39mpdf(X)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Normalize responsibilities\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/ceph-data/jack/conda/lib/python3.9/site-packages/scipy/stats/_multivariate.py:393\u001b[0m, in \u001b[0;36mmultivariate_normal_gen.__call__\u001b[0;34m(self, mean, cov, allow_singular, seed)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cov\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, allow_singular\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    389\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a frozen multivariate normal distribution.\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \n\u001b[1;32m    391\u001b[0m \u001b[38;5;124;03m    See `multivariate_normal_frozen` for more information.\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmultivariate_normal_frozen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mallow_singular\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_singular\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/ceph-data/jack/conda/lib/python3.9/site-packages/scipy/stats/_multivariate.py:834\u001b[0m, in \u001b[0;36mmultivariate_normal_frozen.__init__\u001b[0;34m(self, mean, cov, allow_singular, seed, maxpts, abseps, releps)\u001b[0m\n\u001b[1;32m    791\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create a frozen multivariate normal distribution.\u001b[39;00m\n\u001b[1;32m    792\u001b[0m \n\u001b[1;32m    793\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    830\u001b[0m \n\u001b[1;32m    831\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dist \u001b[38;5;241m=\u001b[39m multivariate_normal_gen(seed)\n\u001b[1;32m    833\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcov_object \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 834\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcov\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_singular\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallow_singular \u001b[38;5;241m=\u001b[39m allow_singular \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcov_object\u001b[38;5;241m.\u001b[39m_allow_singular\n\u001b[1;32m    836\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m maxpts:\n",
      "File \u001b[0;32m/mnt/ceph-data/jack/conda/lib/python3.9/site-packages/scipy/stats/_multivariate.py:417\u001b[0m, in \u001b[0;36mmultivariate_normal_gen._process_parameters\u001b[0;34m(self, mean, cov, allow_singular)\u001b[0m\n\u001b[1;32m    410\u001b[0m dim, mean, cov \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_parameters_psd(\u001b[38;5;28;01mNone\u001b[39;00m, mean, cov)\n\u001b[1;32m    411\u001b[0m \u001b[38;5;66;03m# After input validation, some methods then processed the arrays\u001b[39;00m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;66;03m# with a `_PSD` object and used that to perform computation.\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;66;03m# To avoid branching statements in each method depending on whether\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;66;03m# `cov` is an array or `Covariance` object, we always process the\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;66;03m# array with `_PSD`, and then use wrapper that satisfies the\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;66;03m# `Covariance` interface, `CovViaPSD`.\u001b[39;00m\n\u001b[0;32m--> 417\u001b[0m psd \u001b[38;5;241m=\u001b[39m \u001b[43m_PSD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcov\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_singular\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_singular\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m cov_object \u001b[38;5;241m=\u001b[39m _covariance\u001b[38;5;241m.\u001b[39mCovViaPSD(psd)\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dim, mean, cov_object\n",
      "File \u001b[0;32m/mnt/ceph-data/jack/conda/lib/python3.9/site-packages/scipy/stats/_multivariate.py:167\u001b[0m, in \u001b[0;36m_PSD.__init__\u001b[0;34m(self, M, cond, rcond, lower, check_finite, allow_singular)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmin(s) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39meps:\n\u001b[1;32m    166\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe input matrix must be symmetric positive semidefinite.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 167\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    168\u001b[0m d \u001b[38;5;241m=\u001b[39m s[s \u001b[38;5;241m>\u001b[39m eps]\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(d) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(s) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_singular:\n",
      "\u001b[0;31mValueError\u001b[0m: The input matrix must be symmetric positive semidefinite."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import sqlite3\n",
    "import json\n",
    "from datetime import datetime\n",
    "from numpy.linalg import det, inv, slogdet\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "class OnlineGMM:\n",
    "    def __init__(self, n_components, n_features, init_params=None, forgetting_factor=0.9):\n",
    "        \"\"\"\n",
    "        Initializes the OnlineGMM model.\n",
    "\n",
    "        Parameters:\n",
    "        - n_components: Number of mixture components.\n",
    "        - n_features: Dimensionality of the data.\n",
    "        - init_params: Optional dictionary with initial 'weights', 'means', and 'covariances'.\n",
    "        - forgetting_factor: Forgetting factor λ (between 0 and 1).\n",
    "        \"\"\"\n",
    "        self.n_components = n_components\n",
    "        self.n_features = n_features\n",
    "        self.forgetting_factor = forgetting_factor  # λ\n",
    "        λ = forgetting_factor\n",
    "        if init_params:\n",
    "            self.weights = np.array(init_params['weights'])  # π_k\n",
    "            self.means = np.array(init_params['means'])      # μ_k\n",
    "            self.covariances = np.array(init_params['covariances'])  # Σ_k\n",
    "        else:\n",
    "            # Initialize uniformly if no initial parameters are provided\n",
    "            self.weights = np.ones(n_components) / n_components\n",
    "            self.means = np.random.rand(n_components, n_features)\n",
    "            self.covariances = np.array([np.eye(n_features) for _ in range(n_components)])\n",
    "        # Initialize sufficient statistics\n",
    "        self.N_k = np.zeros(n_components)  # Effective number of points per component\n",
    "        self.M_k = np.zeros((n_components, n_features))  # Weighted sum of data per component\n",
    "        self.S_k = np.zeros((n_components, n_features, n_features))  # Weighted sum of outer products per component\n",
    "\n",
    "    def fit_batch(self, X):\n",
    "        \"\"\"\n",
    "        Processes a new batch of data and updates the model parameters.\n",
    "\n",
    "        Parameters:\n",
    "        - X: New data batch (array-like of shape (batch_size, n_features)).\n",
    "        \"\"\"\n",
    "        batch_size = X.shape[0]\n",
    "        # E-step: Compute responsibilities\n",
    "        responsibilities = np.zeros((batch_size, self.n_components))\n",
    "        for k in range(self.n_components):\n",
    "            print(self.means[k], self.covariances[k])\n",
    "            rv = multivariate_normal(mean=self.means[k], cov=self.covariances[k], allow_singular=True)\n",
    "            responsibilities[:, k] = self.weights[k] * rv.pdf(X)\n",
    "        # Normalize responsibilities\n",
    "        responsibilities_sum = responsibilities.sum(axis=1, keepdims=True)\n",
    "        responsibilities = responsibilities / (responsibilities_sum + 1e-10)\n",
    "        # M-step: Update sufficient statistics with forgetting factor λ\n",
    "        y = responsibilities  # γ_{nk}\n",
    "        N_k_batch = y.sum(axis=0)  # Sum over data points for each component\n",
    "        M_k_batch = y.T @ X        # Weighted sum of data\n",
    "        S_k_batch = np.zeros((self.n_components, self.n_features, self.n_features))\n",
    "        for k in range(self.n_components):\n",
    "            diff = X - self.means[k]\n",
    "            S_k_batch[k] = (y[:, k][:, np.newaxis] * diff).T @ diff\n",
    "\n",
    "        print(N_k_batch, M_k_batch, S_k_batch)\n",
    "        # Apply forgetting factor\n",
    "        λ = self.forgetting_factor\n",
    "        self.N_k = λ * self.N_k + (1 - λ) * N_k_batch\n",
    "        self.M_k = λ * self.M_k + (1 - λ) * M_k_batch\n",
    "        self.S_k = λ * self.S_k + (1 - λ) * S_k_batch\n",
    "        # Update model parameters\n",
    "        self.weights = self.N_k / (self.N_k.sum() + 1e-10)\n",
    "        self.means = self.M_k / self.N_k[:, np.newaxis]\n",
    "        for k in range(self.n_components):\n",
    "            cov_k = self.S_k[k] / self.N_k[k] - np.outer(self.means[k], self.means[k])\n",
    "            # Regularize covariance to ensure positive definiteness\n",
    "            cov_k += 1e-6 * np.eye(self.n_features)\n",
    "            self.covariances[k] = cov_k\n",
    "\n",
    "    def get_params(self):\n",
    "        \"\"\"\n",
    "        Returns the current model parameters.\n",
    "\n",
    "        Returns:\n",
    "        - A dictionary with 'weights', 'means', and 'covariances'.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'weights': self.weights,\n",
    "            'means': self.means,\n",
    "            'covariances': self.covariances\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "def create_raw_data_table(partition_name, data_dims):\n",
    "    db_name = f\"gmm_sketch_{partition_name}.db\"\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    string = '''\n",
    "        CREATE TABLE IF NOT EXISTS raw_data (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            timestamp TEXT,\n",
    "'''\n",
    "\n",
    "    for n in range(data_dims):\n",
    "        if n != data_dims-1:\n",
    "            string += f'            x{n} REAL,\\n'\n",
    "        else:\n",
    "            string += f'            x{n} REAL\\n'\n",
    "\n",
    "    string += '         )'\n",
    "\n",
    "    cursor.execute(string)\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def insert_raw_data(partition_name, data_frame):\n",
    "    db_name = f\"gmm_sketch_{partition_name}.db\"\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    current_time = datetime.now().isoformat()\n",
    "    tmp_df = data_frame.copy()\n",
    "    tmp_df.insert(0, 'timestamp', current_time)\n",
    "    cols = ['timestamp']\n",
    "    for n in range(len(data_frame.columns)):\n",
    "        cols.append(f'x{n}')\n",
    "    tmp_df.columns = cols\n",
    "    tmp_df.to_sql('raw_data', conn, if_exists='append', index=False)\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def store_parameters_with_timestamp(params):\n",
    "    db_name = f\"gmm_sketch_{params['partition']}.db\"\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    # Create table with timestamp\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS gmm_sketch (\n",
    "            timestamp TEXT,\n",
    "            weights TEXT,\n",
    "            means TEXT,\n",
    "            covariances TEXT\n",
    "        )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "    current_time = datetime.now().isoformat()\n",
    "    cursor.execute('''\n",
    "        INSERT INTO gmm_sketch (timestamp, weights, means, covariances)\n",
    "        VALUES (?, ?, ?, ?)\n",
    "    ''', (\n",
    "        current_time,\n",
    "        json.dumps(params['weights']),\n",
    "        json.dumps(params['means']),\n",
    "        json.dumps(params['covariances'])\n",
    "    ))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "'''\n",
    "def process_partition(partition_name, partition_data):\n",
    "    create_raw_data_table(partition_name, len(partition_data.columns))\n",
    "    insert_raw_data(partition_name, partition_data)\n",
    "    X = load_raw_data(partition_name).drop(['id', 'timestamp'], axis=1).values\n",
    "    gmm = GaussianMixture(n_components=20, covariance_type='full', random_state=42)\n",
    "    gmm.fit(X)\n",
    "    params = {\n",
    "        'partition': partition_name,\n",
    "        'weights': gmm.weights_.tolist(),\n",
    "        'means': gmm.means_.tolist(),\n",
    "        'covariances': gmm.covariances_.tolist()\n",
    "    }\n",
    "    store_parameters_with_timestamp(params)\n",
    "    print(f\"GMM for partition {partition_name} has been processed and stored.\")\n",
    "\n",
    "'''\n",
    "online_gmm = OnlineGMM(n_components=20, n_features=10)\n",
    "\n",
    "def process_partition(partition_name, partition_data):\n",
    "    create_raw_data_table(partition_name, len(partition_data.columns))\n",
    "    insert_raw_data(partition_name, partition_data)\n",
    "    X = load_raw_data(partition_name).drop(['id', 'timestamp'], axis=1).values\n",
    "\n",
    "\n",
    "    online_gmm.fit_batch(partition_data.values)\n",
    "\n",
    "    # Store updated parameters\n",
    "    params = {\n",
    "        'partition': partition_name,\n",
    "        'weights': online_gmm.weights.tolist(),\n",
    "        'means': online_gmm.means.tolist(),\n",
    "        'covariances': online_gmm.covariances.tolist()\n",
    "    }\n",
    "    store_parameters_with_timestamp(params)\n",
    "    print(f\"Online GMM for partition {partition_name} has been updated and stored.\")\n",
    "'''\n",
    "def process_partition(partition_name, partition_data):\n",
    "    create_raw_data_table(partition_name, len(partition_data.columns))\n",
    "    insert_raw_data(partition_name, partition_data)\n",
    "    X = load_raw_data(partition_name).drop(['id', 'timestamp'], axis=1).values\n",
    "    print(\"TOTAL: \", X, X.shape)\n",
    "    print(\"NEW: \", partition_data)\n",
    "\n",
    "    # Check if a previous GMM exists\n",
    "    timestamps = get_timestamps(partition_name)\n",
    "    if timestamps:\n",
    "        # Load the latest GMM parameters\n",
    "        latest_timestamp = timestamps[0]\n",
    "        gmm_params = load_gmm_by_timestamp(partition_name, latest_timestamp)\n",
    "        online_gmm = OnlineGMM(n_components=len(gmm_params.weights_), n_features=X.shape[1])\n",
    "        online_gmm.set_params(gmm_params)\n",
    "    else:\n",
    "        # Initialize a new GMM model\n",
    "        online_gmm = OnlineGMM(n_components=20, n_features=X.shape[1], learning_rate=0.001)  # Adjusted learning rate\n",
    "\n",
    "    # Process one data point at a time\n",
    "    for x in partition_data.values:\n",
    "        online_gmm.online_em_step(np.array([x]))\n",
    "\n",
    "    # Store updated parameters\n",
    "    params = {\n",
    "        'partition': partition_name,\n",
    "        'weights': online_gmm.weights.tolist(),\n",
    "        'means': online_gmm.means.tolist(),\n",
    "        'covariances': online_gmm.covariances.tolist()\n",
    "    }\n",
    "    store_parameters_with_timestamp(params)\n",
    "    print(f\"Online GMM for partition {partition_name} has been updated and stored.\")\n",
    "'''\n",
    "\n",
    "\n",
    "def load_raw_data(partition_name):\n",
    "    db_name = f\"gmm_sketch_{partition_name}.db\"\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    df = pd.read_sql_query('SELECT * FROM raw_data', conn)\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "def get_timestamps(partition_name):\n",
    "    db_name = f\"gmm_sketch_{partition_name}.db\"\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Check if the table 'gmm_sketch' exists\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name='gmm_sketch'\")\n",
    "    table_exists = cursor.fetchone()\n",
    "\n",
    "    if not table_exists:\n",
    "        # Table does not exist\n",
    "        conn.close()\n",
    "        return None  # or return False if you prefer\n",
    "    else:\n",
    "        cursor.execute('SELECT timestamp FROM gmm_sketch ORDER BY timestamp DESC')\n",
    "        timestamps = [row[0] for row in cursor.fetchall()]\n",
    "        conn.close()\n",
    "        return timestamps\n",
    "\n",
    "\n",
    "def load_gmm_by_timestamp(partition_name, timestamp):\n",
    "    db_name = f\"gmm_sketch_{partition_name}.db\"\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        SELECT weights, means, covariances FROM gmm_sketch\n",
    "        WHERE timestamp = ?\n",
    "    ''', (timestamp,))\n",
    "    row = cursor.fetchone()\n",
    "    conn.close()\n",
    "    if row:\n",
    "        weights = json.loads(row[0])\n",
    "        means = json.loads(row[1])\n",
    "        covariances = json.loads(row[2])\n",
    "        gmm = reconstruct_gmm(weights, means, covariances)\n",
    "        return gmm\n",
    "    else:\n",
    "        print(f\"No GMM parameters found for partition {partition_name} at timestamp {timestamp}.\")\n",
    "        return None\n",
    "\n",
    "def reconstruct_gmm(weights, means, covariances):\n",
    "    gmm = GaussianMixture(n_components=len(weights), covariance_type='full')\n",
    "    gmm.weights_ = np.array(weights)\n",
    "    gmm.means_ = np.array(means)\n",
    "    gmm.covariances_ = np.array(covariances)\n",
    "    # Compute precisions_cholesky_\n",
    "    gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(gmm.covariances_))\n",
    "    return gmm\n",
    "\n",
    "def cauchy_schwarz_divergence(gmm_p, gmm_q):\n",
    "    weights_p = gmm_p.weights_\n",
    "    means_p = gmm_p.means_\n",
    "    covs_p = gmm_p.covariances_\n",
    "\n",
    "    weights_q = gmm_q.weights_\n",
    "    means_q = gmm_q.means_\n",
    "    covs_q = gmm_q.covariances_\n",
    "\n",
    "    K = len(weights_p)  # Number of components\n",
    "    d = means_p.shape[1]  # Dimensionality\n",
    "\n",
    "    # Precompute constants\n",
    "    coeff = (2 * np.pi) ** (d / 2)\n",
    "\n",
    "    # Compute I_pq\n",
    "    I_pq = 0.0\n",
    "    for i in range(K):\n",
    "        for j in range(K):\n",
    "            w_i = weights_p[i]\n",
    "            w_j = weights_q[j]\n",
    "            mu_i = means_p[i]\n",
    "            mu_j = means_q[j]\n",
    "            Sigma_i = covs_p[i]\n",
    "            Sigma_j = covs_q[j]\n",
    "\n",
    "            Sigma_ij = Sigma_i + Sigma_j\n",
    "            try:\n",
    "                # Compute determinant and inverse\n",
    "                sign, logdet_Sigma_ij = slogdet(Sigma_ij)\n",
    "                if sign <= 0:\n",
    "                    raise np.linalg.LinAlgError(\"Covariance matrix is not positive definite.\")\n",
    "                diff = mu_i - mu_j\n",
    "                exponent = -0.5 * diff.T @ inv(Sigma_ij) @ diff\n",
    "                I_ij_pq = w_i * w_j * np.exp(exponent) / (coeff * np.exp(0.5 * logdet_Sigma_ij))\n",
    "                I_pq += I_ij_pq\n",
    "            except np.linalg.LinAlgError as e:\n",
    "                print(f\"Error computing I_pq for components {i}, {j}: {e}\")\n",
    "                return None\n",
    "\n",
    "    # Compute I_pp\n",
    "    I_pp = 0.0\n",
    "    for i in range(K):\n",
    "        for j in range(K):\n",
    "            w_i = weights_p[i]\n",
    "            w_j = weights_p[j]\n",
    "            mu_i = means_p[i]\n",
    "            mu_j = means_p[j]\n",
    "            Sigma_i = covs_p[i]\n",
    "            Sigma_j = covs_p[j]\n",
    "\n",
    "            Sigma_ij = Sigma_i + Sigma_j\n",
    "            try:\n",
    "                # Compute determinant and inverse\n",
    "                sign, logdet_Sigma_ij = slogdet(Sigma_ij)\n",
    "                if sign <= 0:\n",
    "                    raise np.linalg.LinAlgError(\"Covariance matrix is not positive definite.\")\n",
    "                diff = mu_i - mu_j\n",
    "                exponent = -0.5 * diff.T @ inv(Sigma_ij) @ diff\n",
    "                I_ij_pp = w_i * w_j * np.exp(exponent) / (coeff * np.exp(0.5 * logdet_Sigma_ij))\n",
    "                I_pp += I_ij_pp\n",
    "            except np.linalg.LinAlgError as e:\n",
    "                print(f\"Error computing I_pp for components {i}, {j}: {e}\")\n",
    "                return None\n",
    "\n",
    "    # Compute I_qq\n",
    "    I_qq = 0.0\n",
    "    for i in range(K):\n",
    "        for j in range(K):\n",
    "            w_i = weights_q[i]\n",
    "            w_j = weights_q[j]\n",
    "            mu_i = means_q[i]\n",
    "            mu_j = means_q[j]\n",
    "            Sigma_i = covs_q[i]\n",
    "            Sigma_j = covs_q[j]\n",
    "\n",
    "            Sigma_ij = Sigma_i + Sigma_j\n",
    "            try:\n",
    "                # Compute determinant and inverse\n",
    "                sign, logdet_Sigma_ij = slogdet(Sigma_ij)\n",
    "                if sign <= 0:\n",
    "                    raise np.linalg.LinAlgError(\"Covariance matrix is not positive definite.\")\n",
    "                diff = mu_i - mu_j\n",
    "                exponent = -0.5 * diff.T @ inv(Sigma_ij) @ diff\n",
    "                I_ij_qq = w_i * w_j * np.exp(exponent) / (coeff * np.exp(0.5 * logdet_Sigma_ij))\n",
    "                I_qq += I_ij_qq\n",
    "            except np.linalg.LinAlgError as e:\n",
    "                print(f\"Error computing I_qq for components {i}, {j}: {e}\")\n",
    "                return None\n",
    "\n",
    "    # Compute the Cauchy-Schwarz divergence\n",
    "    cs_divergence = -np.log((I_pq ** 2) / (I_pp * I_qq))\n",
    "\n",
    "    return cs_divergence\n",
    "\n",
    "process_partition('A', X_train.drop(['cc_num', 'merchant', 'category', 'job'], axis=1))\n",
    "#process_partition('A', val_df.sample(100).drop(['cc_num', 'merchant', 'category', 'job', 'is_fraud'], axis=1))\n",
    "process_partition('A', poisoned_df.drop(['cc_num', 'merchant', 'category', 'job', 'is_fraud'], axis=1))\n",
    "\n",
    "# Step 7: Compute divergence between the two latest GMMs\n",
    "timestamps = get_timestamps('A')\n",
    "if len(timestamps) >= 2:\n",
    "    timestamp_current, timestamp_previous = timestamps[0], timestamps[1]\n",
    "    gmm_current = load_gmm_by_timestamp('A', timestamp_current)\n",
    "    gmm_previous = load_gmm_by_timestamp('A', timestamp_previous)\n",
    "    if gmm_current and gmm_previous:\n",
    "        cs_div = cauchy_schwarz_divergence(gmm_current, gmm_previous)\n",
    "        print(f\"Cauchy-Schwarz Divergence between GMMs at {timestamp_current} and {timestamp_previous}: {cs_div}\")\n",
    "else:\n",
    "    print(\"Not enough timestamps to compute divergence.\")\n",
    "\n",
    "# Step 8: Load and display raw data (optional)\n",
    "df_raw = load_raw_data('A')\n",
    "print(f\"Total raw data points in partition A: {len(df_raw)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amt</th>\n",
       "      <th>gender</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>city_pop</th>\n",
       "      <th>unix_time</th>\n",
       "      <th>merch_lat</th>\n",
       "      <th>merch_long</th>\n",
       "      <th>age</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4414</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.19</td>\n",
       "      <td>0.64</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-1.64</td>\n",
       "      <td>-2.32</td>\n",
       "      <td>0.71</td>\n",
       "      <td>-0.73</td>\n",
       "      <td>-1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109326</th>\n",
       "      <td>-0.28</td>\n",
       "      <td>1</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-2.16</td>\n",
       "      <td>0.62</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-2.19</td>\n",
       "      <td>-1.36</td>\n",
       "      <td>-0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219396</th>\n",
       "      <td>0.33</td>\n",
       "      <td>1</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.05</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284761</th>\n",
       "      <td>-0.40</td>\n",
       "      <td>0</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-1.01</td>\n",
       "      <td>-0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464782</th>\n",
       "      <td>-0.10</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>1.43</td>\n",
       "      <td>-0.79</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         amt  gender   lat  long  city_pop  unix_time  merch_lat  merch_long  \\\n",
       "4414    0.48       0 -2.19  0.64     -0.13      -1.64      -2.32        0.71   \n",
       "109326 -0.28       1  0.20 -2.16      0.62      -1.00       0.10       -2.19   \n",
       "219396  0.33       1  0.17  1.05     -0.29      -0.27       0.29        1.11   \n",
       "284761 -0.40       0  0.22  0.03     -0.29       0.23       0.14        0.01   \n",
       "464782 -0.10       0 -0.78 -0.38     -0.29       1.43      -0.79       -0.41   \n",
       "\n",
       "         age  hour  \n",
       "4414   -0.73 -1.06  \n",
       "109326 -1.36 -0.18  \n",
       "219396  0.25  1.87  \n",
       "284761 -1.01 -0.62  \n",
       "464782  0.08  0.55  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.drop(['cc_num', 'merchant', 'category', 'job'], axis=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amt</th>\n",
       "      <th>gender</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>city_pop</th>\n",
       "      <th>unix_time</th>\n",
       "      <th>merch_lat</th>\n",
       "      <th>merch_long</th>\n",
       "      <th>is_fraud</th>\n",
       "      <th>age</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.73</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.63</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0</td>\n",
       "      <td>0.82</td>\n",
       "      <td>-0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.81</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.63</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0</td>\n",
       "      <td>0.82</td>\n",
       "      <td>-0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.68</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.63</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0</td>\n",
       "      <td>0.82</td>\n",
       "      <td>-0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.29</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.63</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0</td>\n",
       "      <td>0.82</td>\n",
       "      <td>-0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.55</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.63</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0</td>\n",
       "      <td>0.82</td>\n",
       "      <td>-0.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   amt  gender   lat  long  city_pop  unix_time  merch_lat  merch_long  \\\n",
       "0 6.73       1 -0.77  0.02     -0.25       1.63      -0.77        0.02   \n",
       "1 6.81       1 -0.77  0.03     -0.25       1.63      -0.77        0.03   \n",
       "2 6.68       1 -0.77  0.04     -0.25       1.63      -0.77        0.02   \n",
       "3 6.29       1 -0.77  0.03     -0.25       1.63      -0.78        0.02   \n",
       "4 6.55       1 -0.77  0.02     -0.25       1.63      -0.77        0.03   \n",
       "\n",
       "   is_fraud  age  hour  \n",
       "0         0 0.82 -0.11  \n",
       "1         0 0.82 -0.16  \n",
       "2         0 0.82 -0.11  \n",
       "3         0 0.82 -0.24  \n",
       "4         0 0.82 -0.15  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poisoned_df.drop(['cc_num', 'merchant', 'category', 'job'], axis=1).head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
